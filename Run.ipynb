{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mechanize\n",
    "import numpy\n",
    "import os\n",
    "import queue\n",
    "import random\n",
    "import shutil\n",
    "import socket\n",
    "import string\n",
    "import threading\n",
    "import time\n",
    "import urllib.request\n",
    "from abc import ABC, abstractmethod\n",
    "import pandas\n",
    "import libcloud\n",
    "import paramiko\n",
    "from dataclasses import dataclass\n",
    "from libcloud.compute.providers import get_driver\n",
    "from libcloud.compute.types import Provider\n",
    "from paramiko.buffered_pipe import PipeTimeout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SSH_USER = ''\n",
    "GCLOUD_ACCOUNT = ''\n",
    "GCLOUD_KEY_PATH = ''  # The path to the Service Account Key (a JSON file)\n",
    "GCLOUD_PROJECT = ''  # GCloud project id\n",
    "DESIGN_CSV = ''  # The CSV with the experiment design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ComputeEngine = get_driver(Provider.GCE)\n",
    "\n",
    "driver = ComputeEngine(GCLOUD_ACCOUNT, GCLOUD_KEY_PATH,\n",
    "                       project=GCLOUD_PROJECT)\n",
    "location = [l for l in driver.list_locations() if l.id == '2000'][0]\n",
    "network = [n for n in driver.ex_list_networks() if n.id ==\n",
    "           '6096184313360012863'][0]\n",
    "\n",
    "try:\n",
    "    # To prevent problems with GCloud reusing IP addresses\n",
    "    os.remove(f'/home/{SSH_USER}/.ssh/known_hosts')\n",
    "except:\n",
    "    pass\n",
    "\n",
    "ex_id = ''.join(random.choice(string.ascii_lowercase)\n",
    "                for i in range(8))  # Generate a random project id\n",
    "EX_RUNTIME = 300  # seconds\n",
    "print(f\"Experiment ID: {ex_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Node(ABC):\n",
    "    def __init__(self, driver, name, master=False, masterNode=None):\n",
    "        self.driver = driver\n",
    "        self.name = name\n",
    "        if not master and masterNode == None:\n",
    "            raise ValueError(\"Slave nodes need a master\")\n",
    "        self.master = masterNode\n",
    "        self.disk = self.driver.create_volume(40, f\"boot-{self.name}\", image='opendl-2', location=location)\n",
    "        self.node = self.driver.create_node(\n",
    "            name, 'n1-standard-1', None, location=location, ex_boot_disk=self.disk)\n",
    "        self.driver.wait_until_running([self.node])\n",
    "        self.pubip = self.node.public_ips[0]\n",
    "        self.privip = self.node.private_ips[0]\n",
    "        self.connected = False\n",
    "\n",
    "        for i in range(5):  # Try 5 times\n",
    "            try:\n",
    "                self.open_ssh()\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                time.sleep(5)\n",
    "        if not self.connected:\n",
    "            raise RuntimeError(f\"Can't connect to node {self.name}\")\n",
    "        self.start_type()\n",
    "\n",
    "    def __del__(self):\n",
    "        self.close_ssh()\n",
    "\n",
    "    def open_ssh(self):\n",
    "        self.ssh = paramiko.SSHClient()\n",
    "        self.ssh.load_system_host_keys()\n",
    "        self.ssh.set_missing_host_key_policy(paramiko.WarningPolicy())\n",
    "        self.ssh.connect(self.pubip, port=22, username=SSH_USER)\n",
    "        self.connected = True\n",
    "\n",
    "    def close_ssh(self):\n",
    "        self.connected = False\n",
    "        self.ssh.close()\n",
    "\n",
    "    @abstractmethod\n",
    "    def start_type(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class JobOptions:\n",
    "    batch_size: int\n",
    "    max_epochs: int\n",
    "\n",
    "\n",
    "class MasterNode(Node):\n",
    "    def start_type(self):\n",
    "        stdin, stdout, stderr = self.ssh.exec_command(f'echo \"SPARK_MASTER_HOST=\\'{self.privip}\\'\" >> /home/{SSH_USER}/bd/spark/conf/spark-env.sh')\n",
    "        if (len(stderr.read()) > 0):\n",
    "            print(stdout.read())\n",
    "            print(stderr.read())\n",
    "\n",
    "        stdin, stdout, stderr = self.ssh.exec_command(\n",
    "            '/home/{SSH_USER}/bd/spark/sbin/start-master.sh')\n",
    "        if (len(stderr.read()) > 0):\n",
    "            print(stdout.read())\n",
    "            print(stderr.read())\n",
    "\n",
    "    def submit(self, options: JobOptions):\n",
    "\n",
    "        stdin, stdout, stderr = self.ssh.exec_command(f\"/home/{SSH_USER}/bd/spark/bin/spark-submit --master {self.privip} --driver-cores 1 \"\n",
    "                                                      f\"--driver-memory 1G --total-executor-cores 1 --executor-cores 1 --executor-memory 1G \"\n",
    "                                                      f\"--py-files \\\"/home/{SSH_USER}/bd/spark/lib/bigdl-0.8.0-python-api,/home/{SSH_USER}/bd/codes/bi-rnn.py\\\" \"\n",
    "                                                      f\"--properties-file \\\"/home/{SSH_USER}/bd/spark/conf/spark-bigdl.conf\\\" \"\n",
    "                                                      f\"--jars \\\"/home/{SSH_USER}/bd/spark/lib/bigdl-SPARK_2.3-0.8.0-jar-with-dependencies.jar\\\" \"\n",
    "                                                      f\"--conf \\\"spark.driver.extraClassPath=/home/{SSH_USER}/bd/spark/lib/bigdl-SPARK_2.3-0.8.0-jar-with-dependencies.jar\\\" \"\n",
    "                                                      f\"--conf \\\"spark.executer.extraClassPath=bigdl-SPARK_2.3-0.8.0-jar-with-dependencies.jar /home/{SSH_USER}/bd/codes/bi-rnn.py\\\" \"\n",
    "                                                      f\"--action train --dataPath \\\"/tmp/mnist\\\" --batchSize {options.batch_size} --endTriggerNum {options.max_epochs} \"\n",
    "                                                      f\"--learningRate 0.01 --learningrateDecay 0.0002\")\n",
    "        if (len(stderr.read()) > 0):\n",
    "            print(stdout.read())\n",
    "            print(stderr.read())\n",
    "\n",
    "    def cancel(self):\n",
    "        br = mechanize.Browser()\n",
    "        br.open(f\"http://{self.pubip}:8080\")\n",
    "\n",
    "        def select_form(form):\n",
    "            return form.attrs.get('action', None) == 'app/kill/'\n",
    "        try:\n",
    "            br.select_form(predicate=select_form)\n",
    "        except mechanize._mechanize.FormNotFoundError:\n",
    "            print(\"FormNotFoundError\")\n",
    "        except Exception as e:\n",
    "            print(\"An error occurred during cancelling.\")\n",
    "            print(e)\n",
    "        br.submit()\n",
    "\n",
    "\n",
    "class SlaveNode(Node):\n",
    "    def start_type(self):\n",
    "        stdin, stdout, stderr = self.ssh.exec_command(f'/home/{SSH_USER}/bd/spark/sbin/start-slave.sh spark://{self.master.privip}:7077')\n",
    "        if (len(stderr.read()) > 0):\n",
    "            print(stdout.read())\n",
    "            print(stderr.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master = MasterNode(driver, f\"master-{ex_id}-1\", master=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments = pandas.read_csv(DESIGN_CSV)\n",
    "experiments.columns.values[0] = 'Index'\n",
    "experiments.set_index('Index')\n",
    "\n",
    "if 'failure_rate' not in experiments:\n",
    "    experiments['failure_rate'] = 0\n",
    "if 'failure_duration' not in experiments:\n",
    "    experiments['failure_duration'] = 0\n",
    "\n",
    "print(experiments)\n",
    "num_slaves = max(experiments['num_nodes'])\n",
    "print(f\"Number of slaves: {num_slaves}\")\n",
    "\n",
    "os.makedirs(f\"raw/{ex_id}\", exist_ok=True)\n",
    "shutil.copyfile(DESIGN_CSV, f\"raw/{ex_id}/design.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "slaves = [SlaveNode(driver, f\"slave-{ex_id}-{i}\", masterNode=master) for i in range(1, int(num_slaves)+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, row in experiments.iterrows():\n",
    "    print(f\"Experiment {row['Index']}\")\n",
    "    options = JobOptions(int(row['batch_size']), 50)\n",
    "\n",
    "    for slave in slaves:\n",
    "        slave.start_failure_worker(\n",
    "            row['failure_rate'], row['failure_duration'])\n",
    "\n",
    "    filename = f\"{int(row.Index)}-nodes{int(row.num_nodes)}-batch{options.batch_size}-epochs{options.max_epochs}-frate{row.failure_rate}-duration{row.failure_duration}-time{EX_RUNTIME}.log\"\n",
    "\n",
    "    stdin, stdout, stderr = master.ssh.exec_command(f\"/home/{SSH_USER}/bd/spark/bin/spark-submit --master spark://{master.privip}:7077 --driver-cores 1 \"\n",
    "                                                    f\"--driver-memory 1G --total-executor-cores {int(row.num_nodes)} --executor-cores 1 --executor-memory {int(row.memory_size)}M \"\n",
    "                                                    f\"--py-files /home/{SSH_USER}/bd/spark/lib/bigdl-0.8.0-python-api,/home/{SSH_USER}/bd/codes/bi-rnn.py \"\n",
    "                                                    f\"--properties-file /home/{SSH_USER}/bd/spark/conf/spark-bigdl.conf \"\n",
    "                                                    f\"--jars /home/{SSH_USER}/bd/spark/lib/bigdl-SPARK_2.3-0.8.0-jar-with-dependencies.jar \"\n",
    "                                                    f\"--conf spark.driver.extraClassPath=/home/{SSH_USER}/bd/spark/lib/bigdl-SPARK_2.3-0.8.0-jar-with-dependencies.jar \"\n",
    "                                                    f\"--conf spark.executer.extraClassPath=bigdl-SPARK_2.3-0.8.0-jar-with-dependencies.jar /home/{SSH_USER}/bd/codes/bi-rnn.py \"\n",
    "                                                    f\"--action train --dataPath /tmp/mnist --batchSize {options.batch_size} --endTriggerNum {options.max_epochs} \"\n",
    "                                                    f\"--learningRate 0.01 --learningrateDecay 0.0002 > {ex_id}-{filename}\", timeout=EX_RUNTIME)\n",
    "\n",
    "    try:\n",
    "        print(stdout.read())\n",
    "        print(stderr.read())\n",
    "    except PipeTimeout:\n",
    "        print(\"PipeTimeout\")\n",
    "    except socket.timeout:\n",
    "        print(\"Socket timeout\")\n",
    "    master.cancel()\n",
    "    sftp = master.ssh.open_sftp()\n",
    "    sftp.get(f'{ex_id}-{filename}', f'raw/{ex_id}/{filename}')\n",
    "\n",
    "    for slave in slaves:\n",
    "        slave.stop_failure_worker()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
